{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIXi0Yg1df/Ggfl0n38dGR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Funky-Synatra/Literary_Style_Models/blob/main/Copy_of_Literary_Style_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKZcl0BpNUu_"
      },
      "outputs": [],
      "source": [
        "# Google Colab Notebook\n",
        "\n",
        "# Etapa 1: Configuração do ambiente\n",
        "!pip install transformers datasets\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
        "\n",
        "# Verificar se GPU está disponível\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Funções para carregar o dataset e o data collator\n",
        "def load_dataset(file_path, tokenizer, block_size=128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=file_path,\n",
        "        block_size=block_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "def get_data_collator(tokenizer):\n",
        "    return DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,\n",
        "    )\n",
        "\n",
        "# Etapa 2: Configuração para diferentes línguas e estilos\n",
        "languages = ['portuguese', 'english']\n",
        "styles = ['comedy', 'drama', 'romance']\n",
        "file_paths = {\n",
        "    'portuguese': {\n",
        "        'comedy': \"/content/portuguese_comedy.txt\",\n",
        "        'drama': \"/content/portuguese_drama.txt\",\n",
        "        'romance': \"/content/portuguese_romance.txt\"\n",
        "    },\n",
        "    'english': {\n",
        "        'comedy': \"/content/english_comedy.txt\",\n",
        "        'drama': \"/content/english_drama.txt\",\n",
        "        'romance': \"/content/english_romance.txt\"\n",
        "    }\n",
        "}\n",
        "model_paths = {\n",
        "    'portuguese': {\n",
        "        'comedy': \"./results_portuguese_comedy\",\n",
        "        'drama': \"./results_portuguese_drama\",\n",
        "        'romance': \"./results_portuguese_romance\"\n",
        "    },\n",
        "    'english': {\n",
        "        'comedy': \"./results_english_comedy\",\n",
        "        'drama': \"./results_english_drama\",\n",
        "        'romance': \"./results_english_romance\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Carregar o tokenizador\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Função para treinar o modelo em um estilo específico\n",
        "def train_model(language, style):\n",
        "    print(f\"Treinando modelo para {language} - {style}...\")\n",
        "\n",
        "    # Carregar o dataset específico da língua e estilo\n",
        "    new_dataset = load_dataset(file_paths[language][style], tokenizer)\n",
        "\n",
        "    # Verificar se o modelo já existe\n",
        "    model_path = model_paths[language][style]\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"Carregando modelo existente para {language} - {style}...\")\n",
        "        model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
        "    else:\n",
        "        print(f\"Iniciando um novo modelo do zero para {language} - {style}...\")\n",
        "        config = GPT2Config()\n",
        "        model = GPT2LMHeadModel(config).to(device)\n",
        "\n",
        "    # Definir argumentos de treinamento\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=model_path,\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=5,  # Pode ajustar o número de épocas\n",
        "        per_device_train_batch_size=2,  # Pode ajustar dependendo dos recursos disponíveis\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "    )\n",
        "\n",
        "    # Continuar o treinamento com o novo dataset\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=get_data_collator(tokenizer),\n",
        "        train_dataset=new_dataset,\n",
        "    )\n",
        "\n",
        "    # Iniciar o treinamento com novos dados\n",
        "    trainer.train()\n",
        "\n",
        "    # Salvar o modelo após o treinamento\n",
        "    model.save_pretrained(model_path)\n",
        "    tokenizer.save_pretrained(model_path)\n",
        "\n",
        "    print(f\"Treinamento do modelo para {language} - {style} concluído.\")\n",
        "\n",
        "# Treinar modelos para cada combinação de língua e estilo\n",
        "for language in languages:\n",
        "    for style in styles:\n",
        "        train_model(language, style)\n",
        "\n",
        "# Função para gerar texto com um modelo específico\n",
        "def generate_text(prompt, language, style, max_length=100):\n",
        "    model_path = model_paths[language][style]\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "    model.eval()\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(inputs.input_ids, max_length=max_length, num_return_sequences=1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Testar o gerador de texto para diferentes línguas e estilos\n",
        "prompt = \"Era uma vez\"\n",
        "for language in languages:\n",
        "    for style in styles:\n",
        "        print(f\"Texto gerado para {language} - {style}:\")\n",
        "        generated_text = generate_text(prompt, language, style)\n",
        "        print(generated_text)\n",
        "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
      ]
    }
  ]
}